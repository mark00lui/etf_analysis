#!/usr/bin/env python3
"""
å…ƒå¤§ETFçµ±ä¸€æŠ“å–ç³»çµ±
æ”¯æŒæ‰€æœ‰å…ƒå¤§ETFç”¢å“çš„æ•¸æ“šæŠ“å–å’Œå…¥åº«
"""

import os
import time
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
import pandas as pd
from models.etf_data import ETFDataManager
from utils.logger import setup_logger

class YuantaETFScraper:
    """å…ƒå¤§ETFæŠ“å–å™¨"""
    
    def __init__(self):
        self.logger = setup_logger("yuanta_scraper", "logs/yuanta_scraper.log")
        self.etf_manager = ETFDataManager()
        self.download_dir = os.path.join(os.getcwd(), "downloads", "yuanta")
        
        # ç¢ºä¿ä¸‹è¼‰ç›®éŒ„å­˜åœ¨
        os.makedirs(self.download_dir, exist_ok=True)
        
        # å…ƒå¤§ETFåˆ—è¡¨
        self.etf_list = [
            "0050", "0051", "0053", "0055", "0056", 
            "006201", "006203", "00713", "00850", "00940"
        ]
        
        # åŸºç¤URLæ¨¡æ¿
        self.base_url = "https://www.yuantaetfs.com/product/detail/{}/ratio"
        
        # é‡è©¦è¨­å®š
        self.max_retries = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
        self.retry_delay = 5  # é‡è©¦é–“éš”(ç§’)
        self.download_timeout = 30  # ä¸‹è¼‰è¶…æ™‚æ™‚é–“(ç§’)
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_attempts': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'retry_count': 0
        }
    
    def setup_chrome_driver(self, attempt=1):
        """è¨­ç½®Chromeç€è¦½å™¨é©…å‹• (å¸¶éš¨æ©ŸåŒ–)"""
        chrome_options = Options()
        
        # æ€§èƒ½å„ªåŒ–é¸é …
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--disable-software-rasterizer")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-features=TranslateUI")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")
        
        # éš¨æ©ŸåŒ–è¦–çª—å¤§å° (é¿å…è¢«æª¢æ¸¬)
        import random
        window_sizes = ["1280,720", "1366,768", "1920,1080", "1440,900"]
        chrome_options.add_argument(f"--window-size={random.choice(window_sizes)}")
        
        # éš¨æ©ŸåŒ–User-Agent
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
        ]
        chrome_options.add_argument(f"--user-agent={random.choice(user_agents)}")
        
        # è¨­ç½®ä¸‹è¼‰ç›®éŒ„
        prefs = {
            "download.default_directory": self.download_dir,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": False,
            "profile.default_content_setting_values": {
                "images": 2,
                "plugins": 2,
                "popups": 2,
                "geolocation": 2,
                "notifications": 2,
                "media_stream": 2,
            }
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        # ç¦ç”¨æ—¥èªŒ
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--silent")
        
        try:
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(30)
            driver.implicitly_wait(5)
            return driver
        except Exception as e:
            self.logger.error(f"ç„¡æ³•å•Ÿå‹•Chromeé©…å‹• (ç¬¬{attempt}æ¬¡å˜—è©¦): {e}")
            return None
    
    def extract_date_from_csv(self, lines):
        """å¾CSVç¬¬ä¸€è¡Œæå–æ—¥æœŸ"""
        try:
            if lines and len(lines) > 0:
                first_line = lines[0].strip()
                date_pattern = r'(\d{4}/\d{1,2}/\d{1,2})'
                match = re.search(date_pattern, first_line)
                if match:
                    date_str = match.group(1)
                    date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    return date_obj.strftime('%Y-%m-%d')
            return None
        except Exception as e:
            self.logger.error(f"æå–æ—¥æœŸæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def download_etf_data(self, etf_code):
        """ä¸‹è¼‰å–®ä¸€ETFçš„æ•¸æ“š (å¸¶é‡è©¦æ©Ÿåˆ¶)"""
        url = self.base_url.format(etf_code)
        
        for attempt in range(1, self.max_retries + 1):
            self.stats['total_attempts'] += 1
            self.logger.info(f"ğŸ”„ é–‹å§‹ä¸‹è¼‰ETF {etf_code} æ•¸æ“š (ç¬¬{attempt}æ¬¡å˜—è©¦)")
            
            driver = None
            try:
                driver = self.setup_chrome_driver(attempt)
                if not driver:
                    self.logger.error(f"âŒ ç„¡æ³•å•Ÿå‹•Chromeé©…å‹• (ç¬¬{attempt}æ¬¡å˜—è©¦)")
                    if attempt < self.max_retries:
                        import random
                        delay = self.retry_delay + random.randint(1, 3)
                        self.logger.info(f"â³ å°‡åœ¨{delay}ç§’å¾Œé‡è©¦...")
                        time.sleep(delay)
                        continue
                    return False
                
                # éš¨æ©Ÿå»¶é² (é¿å…è¢«æª¢æ¸¬)
                import random
                time.sleep(random.uniform(1, 3))
                
                # è¨ªå•é é¢
                self.logger.info(f"ğŸŒ æ­£åœ¨è¨ªå•: {url}")
                driver.get(url)
                
                # ç­‰å¾…é é¢åŠ è¼‰
                wait = WebDriverWait(driver, 15)  # å¢åŠ ç­‰å¾…æ™‚é–“
                
                # å°‹æ‰¾ã€ŒåŒ¯å‡ºexcelã€æŒ‰éˆ• (æ›´å¤šé¸æ“‡å™¨)
                excel_button_selectors = [
                    "//span[contains(text(), 'åŒ¯å‡ºexcel')]",
                    "//button[contains(text(), 'åŒ¯å‡ºexcel')]",
                    "//a[contains(text(), 'åŒ¯å‡ºexcel')]",
                    "//div[contains(text(), 'åŒ¯å‡ºexcel')]",
                    "//span[contains(text(), 'åŒ¯å‡ºExcel')]",
                    "//button[contains(text(), 'åŒ¯å‡ºExcel')]",
                    "//a[contains(text(), 'åŒ¯å‡ºExcel')]",
                    "//div[contains(text(), 'åŒ¯å‡ºExcel')]",
                    "//span[contains(text(), 'EXCEL')]",
                    "//button[contains(text(), 'EXCEL')]",
                ]
                
                excel_button = None
                for selector in excel_button_selectors:
                    try:
                        excel_button = wait.until(EC.element_to_be_clickable((By.XPATH, selector)))
                        self.logger.info(f"âœ… æ‰¾åˆ°æŒ‰éˆ•: {selector}")
                        break
                    except TimeoutException:
                        continue
                
                if not excel_button:
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ã€ŒåŒ¯å‡ºexcelã€æŒ‰éˆ•: {etf_code} (ç¬¬{attempt}æ¬¡å˜—è©¦)")
                    if attempt < self.max_retries:
                        import random
                        delay = self.retry_delay + random.randint(1, 3)
                        self.logger.info(f"â³ å°‡åœ¨{delay}ç§’å¾Œé‡è©¦...")
                        time.sleep(delay)
                        continue
                    return False
                
                # è¨˜éŒ„ä¸‹è¼‰å‰çš„æ–‡ä»¶åˆ—è¡¨
                files_before = set(os.listdir(self.download_dir))
                
                # é»æ“ŠæŒ‰éˆ•
                driver.execute_script("arguments[0].click();", excel_button)
                
                # ç­‰å¾…ä¸‹è¼‰å®Œæˆ
                check_interval = 1
                waited_time = 0
                
                while waited_time < self.download_timeout:
                    time.sleep(check_interval)
                    waited_time += check_interval
                    
                    files_after = set(os.listdir(self.download_dir))
                    new_files = files_after - files_before
                    
                    if new_files:
                        self.logger.info(f"âœ… ETF {etf_code} ä¸‹è¼‰å®Œæˆï¼Œæ–°æ–‡ä»¶: {new_files}")
                        self.stats['successful_downloads'] += 1
                        return True
                    
                    if waited_time % 5 == 0:
                        self.logger.info(f"â³ ç­‰å¾…ä¸‹è¼‰ä¸­... ({waited_time}/{self.download_timeout}ç§’)")
                
                self.logger.warning(f"â° ETF {etf_code} ä¸‹è¼‰è¶…æ™‚ (ç¬¬{attempt}æ¬¡å˜—è©¦)")
                if attempt < self.max_retries:
                    self.stats['retry_count'] += 1
                    import random
                    delay = self.retry_delay + random.randint(1, 3)
                    self.logger.info(f"â³ å°‡åœ¨{delay}ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                    continue
                return False
                
            except (WebDriverException, TimeoutException) as e:
                self.logger.error(f"âŒ ä¸‹è¼‰ETF {etf_code} æ™‚ç™¼ç”ŸéŒ¯èª¤ (ç¬¬{attempt}æ¬¡å˜—è©¦): {e}")
                if attempt < self.max_retries:
                    self.stats['retry_count'] += 1
                    import random
                    delay = self.retry_delay + random.randint(1, 3)
                    self.logger.info(f"â³ å°‡åœ¨{delay}ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                    continue
                return False
            except Exception as e:
                self.logger.error(f"âŒ ä¸‹è¼‰ETF {etf_code} æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ (ç¬¬{attempt}æ¬¡å˜—è©¦): {e}")
                if attempt < self.max_retries:
                    self.stats['retry_count'] += 1
                    import random
                    delay = self.retry_delay + random.randint(1, 3)
                    self.logger.info(f"â³ å°‡åœ¨{delay}ç§’å¾Œé‡è©¦...")
                    time.sleep(delay)
                    continue
                return False
            finally:
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
        
        self.logger.error(f"âŒ ETF {etf_code} ä¸‹è¼‰å¤±æ•—ï¼Œå·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ ({self.max_retries})")
        self.stats['failed_downloads'] += 1
        return False
    
    def analyze_csv_file(self, file_path, etf_code):
        """åˆ†æCSVæ–‡ä»¶ä¸¦æå–æ•¸æ“š"""
        try:
            self.logger.info(f"åˆ†ææ–‡ä»¶: {file_path}")
            
            # è®€å–æ–‡ä»¶å…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æå–æ—¥æœŸ
            date = self.extract_date_from_csv(lines)
            if not date:
                date = datetime.now().strftime('%Y-%m-%d')
            
            self.logger.info(f"æå–åˆ°æ—¥æœŸ: {date}")
            
            # å°‹æ‰¾è‚¡ç¥¨æ•¸æ“šè¡¨æ ¼
            stock_data_start = None
            for i, line in enumerate(lines):
                if 'å•†å“ä»£ç¢¼,å•†å“åç¨±,å•†å“æ•¸é‡,å•†å“æ¬Šé‡' in line:
                    stock_data_start = i
                    break
            
            if stock_data_start is None:
                self.logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨æ•¸æ“šè¡¨æ ¼: {etf_code}")
                return False
            
            # æå–è‚¡ç¥¨æ•¸æ“š
            stock_lines = []
            for i in range(stock_data_start + 1, len(lines)):
                line = lines[i].strip()
                if line and ',' in line and not line.startswith('æœŸè²¨'):
                    parts = line.split(',')
                    if len(parts) >= 4 and parts[0].isdigit():
                        stock_lines.append(line)
                    elif 'æœŸè²¨' in line:
                        break
            
            if not stock_lines:
                self.logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨æ•¸æ“š: {etf_code}")
                return False
            
            # æº–å‚™æŒè‚¡æ•¸æ“š
            holdings = []
            for line in stock_lines:
                parts = line.split(',')
                if len(parts) >= 4:
                    holding = {
                        'stock_code': parts[0],
                        'stock_name': parts[1],
                        'shares': int(parts[2]) if parts[2].isdigit() else 0,
                        'weight': float(parts[3])
                    }
                    holdings.append(holding)
            
            # æª¢æŸ¥é‡è¤‡æ•¸æ“š
            duplicate_check = self.etf_manager.check_duplicate_holdings(etf_code, holdings, date)
            self.logger.info(f"ETF {etf_code} é‡è¤‡æª¢æŸ¥: {duplicate_check['message']}")
            
            if duplicate_check['is_duplicate']:
                self.logger.info(f"ETF {etf_code} æ•¸æ“šé‡è¤‡ï¼Œè·³éä¿å­˜")
                return True
            
            # ä¿å­˜åˆ°MongoDB
            success = self.etf_manager.save_holdings(etf_code, holdings, date)
            
            if success:
                self.logger.info(f"ETF {etf_code} æ•¸æ“šä¿å­˜æˆåŠŸ: {len(holdings)} ç­†æŒè‚¡æ•¸æ“š")
                return True
            else:
                self.logger.error(f"ETF {etf_code} æ•¸æ“šä¿å­˜å¤±æ•—")
                return False
                
        except Exception as e:
            self.logger.error(f"åˆ†æETF {etf_code} æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def scrape_all_etfs(self):
        """æŠ“å–æ‰€æœ‰ETFæ•¸æ“š (å¸¶é‡è©¦æ©Ÿåˆ¶)"""
        self.logger.info("ğŸš€ é–‹å§‹æŠ“å–æ‰€æœ‰å…ƒå¤§ETFæ•¸æ“š")
        
        results = {}
        total_start_time = time.time()
        
        for i, etf_code in enumerate(self.etf_list, 1):
            self.logger.info(f"ğŸ“Š è™•ç†ETF {etf_code} ({i}/{len(self.etf_list)})")
            
            etf_start_time = time.time()
            
            # ä¸‹è¼‰æ•¸æ“š (å·²åŒ…å«é‡è©¦æ©Ÿåˆ¶)
            download_success = self.download_etf_data(etf_code)
            
            if download_success:
                # æ‰¾åˆ°æœ€æ–°ä¸‹è¼‰çš„æ–‡ä»¶
                files = os.listdir(self.download_dir)
                csv_files = [f for f in files if f.endswith('.csv')]
                
                if csv_files:
                    latest_file = max([os.path.join(self.download_dir, f) for f in csv_files], 
                                     key=os.path.getctime)
                    
                    # åˆ†ææ–‡ä»¶
                    analysis_success = self.analyze_csv_file(latest_file, etf_code)
                    
                    if analysis_success:
                        results[etf_code] = "âœ… æˆåŠŸ"
                        self.logger.info(f"âœ… ETF {etf_code} è™•ç†å®Œæˆ")
                    else:
                        results[etf_code] = "âŒ åˆ†æå¤±æ•—"
                        self.logger.error(f"âŒ ETF {etf_code} åˆ†æå¤±æ•—")
                else:
                    results[etf_code] = "âŒ æ–‡ä»¶æœªæ‰¾åˆ°"
                    self.logger.error(f"âŒ ETF {etf_code} ä¸‹è¼‰æ–‡ä»¶æœªæ‰¾åˆ°")
            else:
                results[etf_code] = "âŒ ä¸‹è¼‰å¤±æ•—"
                self.logger.error(f"âŒ ETF {etf_code} ä¸‹è¼‰å¤±æ•—")
            
            etf_time = time.time() - etf_start_time
            self.logger.info(f"â±ï¸ ETF {etf_code} è™•ç†è€—æ™‚: {etf_time:.2f}ç§’")
            
        # é¿å…è«‹æ±‚éæ–¼é »ç¹ (éš¨æ©Ÿå»¶é²)
        if i < len(self.etf_list):
            import random
            delay = random.uniform(2, 5)
            time.sleep(delay)
        
        total_time = time.time() - total_start_time
        
        # è¼¸å‡ºçµæœæ‘˜è¦
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“‹ æŠ“å–çµæœæ‘˜è¦")
        self.logger.info("=" * 60)
        
        success_count = sum(1 for status in results.values() if "âœ…" in status)
        failed_count = len(self.etf_list) - success_count
        
        for etf_code, status in results.items():
            self.logger.info(f"ETF {etf_code}: {status}")
        
        self.logger.info(f"â±ï¸ ç¸½è€—æ™‚: {total_time:.2f}ç§’")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count}/{len(self.etf_list)}")
        self.logger.info(f"âŒ å¤±æ•—: {failed_count}/{len(self.etf_list)}")
        
        # çµ±è¨ˆè³‡è¨Š
        self.logger.info("ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
        self.logger.info(f"  ç¸½å˜—è©¦æ¬¡æ•¸: {self.stats['total_attempts']}")
        self.logger.info(f"  æˆåŠŸä¸‹è¼‰: {self.stats['successful_downloads']}")
        self.logger.info(f"  å¤±æ•—ä¸‹è¼‰: {self.stats['failed_downloads']}")
        self.logger.info(f"  é‡è©¦æ¬¡æ•¸: {self.stats['retry_count']}")
        
        if failed_count > 0:
            self.logger.warning(f"âš ï¸ æœ‰ {failed_count} å€‹ETFæŠ“å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
        
        self.logger.info("=" * 60)
        
        return results

def main():
    """ä¸»å‡½æ•¸"""
    scraper = YuantaETFScraper()
    results = scraper.scrape_all_etfs()
    
    print("æŠ“å–å®Œæˆï¼")
    print("çµæœæ‘˜è¦:")
    for etf_code, status in results.items():
        print(f"  {etf_code}: {status}")

if __name__ == "__main__":
    main()
